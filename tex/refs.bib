@ARTICLE{Bolz2003,
  ANNOTATION = {Cited by 895.},
  AUTHOR = {Bolz, Jeff and Farmer, Ian and Grinspun, Eitan and Schroder, Peter},
  FILE = {:Users/cec/Google Drive/Mendeley/2003 - Bolz et al.pdf:pdf},
  JOURNALTITLE = {TOG},
  KEYWORDS = {Conjugate Gradient,Fluid Simulation,GPU Computing,Mesh Smoothing,Multigrid,Navier-Stokes,Numerical Simulation},
  NUMBER = {3},
  PAGES = {917--924},
  TITLE = {{ Sparse matrix solvers on the GPU: conjugate gradients and multigrid }},
  VOLUME = {22},
  YEAR = {2003},
}

@ARTICLE{Stone2010,
  AUTHOR = {Stone, John E. and Gohara, David and Shi, Guochun},
  JOURNALTITLE = {CS { \& } E},
  NUMBER = {3},
  PAGES = {66--73},
  TITLE = {{ OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems }},
  VOLUME = {12},
  YEAR = {2010},
}

@INPROCEEDINGS{Steuwer2011,
  ABSTRACT = {While CUDA and OpenCL made general-purpose programming for Graphics Processing Units (GPU) popular, using these programming approaches remains complex and error-prone because they lack high-level abstractions. The especially challenging systems with multiple GPU are not addressed at all by these low-level programming models. We propose SkelCL â€“ a library providing so-called algorithmic skeletons that capture recurring patterns of parallel compu- tation and communication, together with an abstract vector data type and constructs for specifying data distribution. We demonstrate that SkelCL greatly simplifies programming GPU systems. We report the competitive performance results of SkelCL using both a simple Mandelbrot set computation and an industrial-strength medical imaging application. Because the library is implemented using OpenCL, it is portable across GPU hardware of different vendors.},
  ANNOTATION = {SkelCL is a skeleton library for programming GPUs with OpenCL. The library supports programming of multiple GPUs and implements four data-parallel skeletons: Map, Zip, Reduce, and Scan. The skeleton abstractions create a { \~ { } } 5 { \% } overhead in performance, with a reduction in host program code size of { \~ { } } 4-20x. The design of the skeleton library seems solid. Particular nice features include the ability to supply muscle functions which accept arbitrary extra arguments, for example, a map function which accepts two arguments instead of one. Muscle functions are supplied as strings, which seems incredibly unsafe and prone to error, and significantly reduces the ability of editors/IDEs to detect errors.},
  AUTHOR = {Steuwer, Michel and Kegel, Philipp and Gorlatch, Sergei},
  BOOKTITLE = {IPDPSW},
  DOI = {10.1109/IPDPS.2011.269},
  FILE = {:Users/cec/Google Drive/Mendeley/2011 - Steuwer, Kegel, Gorlatch.pdf:pdf},
  ISBN = {978-1-61284-425-1},
  KEYWORDS = {Algorithmic Skeletons,CUDA,GPU Computing,GPU Programming,Multi-GPU Systems,OpenCL,SkelCL},
  MONTH = {may},
  PAGES = {1176--1182},
  PUBLISHER = {IEEE},
  TITLE = {{ SkelCL - A Portable Skeleton Library for High-Level GPU Programming }},
  YEAR = {2011},
}

@BOOK{Han2011,
  AUTHOR = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  PUBLISHER = {Elsevier},
  TITLE = {{ Data mining: concepts and techniques }},
  YEAR = {2011},
}

@THESIS{Breiman1999,
  ABSTRACT = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Freund and Schapire[1996]), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  ANNOTATION = {Cited by { > } 9000! + dated meme},
  AUTHOR = {Breiman, Leo},
  DOI = {10.1023/A:1010933404324},
  FILE = {:Users/cec/Google Drive/Mendeley/1999 - Breiman.pdf:pdf},
  ISBN = {9781424444427},
  ISSN = {0885-6125},
  PMID = {20142443},
  TITLE = {{ Random forest }},
  TYPE = {phdthesis},
  YEAR = {1999},
}

@MISC{IntelCorporation2012,
  AUTHOR = {{ Intel Corporation }},
  TITLE = {{ OpenCL* Optimization Guide }},
  URL = {https://software.intel.com/sites/landingpage/opencl/optimization-guide/index.htm},
  YEAR = {2012},
}

@INPROCEEDINGS{Ganapathi2009,
  ANNOTATION = {This paper argues for applying statistical machine learning (SML) techniques to develop autotuners for multicore software. They present an autotuner for Stencil codes which can achieve performance within 1 { \% } or up to 18 { \% } better than that of a human expert after 2 hours of running. They evaluate the performance of a randomly selected 1500 configurations (from a posible 10 million configs), and use Kernel Canonical Correlation Analysis (KCCA) to build correlations between tunable parameter values and measured performance values. Performance is measured using hardware counters (L1 cache misses, TLB misses, cycles per thread) and power consumtion in Watts/sec. KCCA seems like a strange choice: it scales exponentially with the feature vector sizes, and it takes 2 hours (!!!) to build the ML model for 400 sec worth of benchmark data. They present an interesting argument that enegy efficiency should be used as an autotuning target as well as just run time, since it was the power wall that lead to the multicore revolution in the first place. They explain the motivation and results well. I like that they compare their results with human expert and hardware upper bound. It is a solid paper which makes a compelling argument, but their choice of only 2 benchmarks and 2 platforms makes the evaluation of their autotuner a little limited. Cited by 58.},
  AUTHOR = {Ganapathi, Archana and Datta, Kaushik and Fox, Armando and Patterson, David},
  BOOKTITLE = {HotPar},
  FILE = {:Users/cec/Google Drive/Mendeley/2009 - Ganapathi et al.pdf:pdf},
  TITLE = {{ A Case for Machine Learning to Optimize Multicore Performance }},
  YEAR = {2009},
}

@INPROCEEDINGS{Zhang2013a,
  ABSTRACT = {This paper develops and evaluates search and optimization techniques for auto-tuning 3D stencil (nearest-neighbor) computations on GPUs. Observations indicate that parameter tuning is necessary for heterogeneous GPUs to achieve optimal performance with respect to a search space. Our proposed framework takes a most concise specification of stencil behavior from the user as a single formula, auto-generates tunable code from it, systematically searches for the best configuration and generates the code with optimal parameter configurations for different GPUs. This auto-tuning approach guarantees adaptive performance for different generations of GPUs while greatly enhancing programmer productivity. Experimental results show that the delivered floating point performance is very close to previous handcrafted work and outperforms other auto-tuned stencil codes by a large margin.},
  ANNOTATION = {A DSL for 3D stencil codes, with an autotuner to search for optimal values of parameters. Parameters are: block size (i.e. decomposing the grid into smaller sections), block dimension (i.e. decomposing blocks into workgroups), and whether to use texture memory or not. This provides a search space of worse case 200 permutations, which are exhaustively evaluated to find the optimum. Since CUDA is statically compiled, this means compiling and evaluating the performance of up to 200 versions of a kernel. Cited by 51.},
  AUTHOR = {Zhang, Yongpeng and Mueller, Frank},
  BOOKTITLE = {CGO},
  DOI = {10.1109/TPDS.2012.160},
  FILE = {:Users/cec/Google Drive/Mendeley/2012 - Zhang, Mueller.pdf:pdf},
  ISBN = {9781450312066},
  ISSN = {10459219},
  PAGES = {155--164},
  TITLE = {{ Auto-generation and Auto-tuning of 3D Stencil Codes on GPU clusters }},
  YEAR = {2012},
}

@INPROCEEDINGS{Nugteren2015,
  ANNOTATION = {A generic iterative search autotuner for OpenCL kernels. Basically OpenTuner for OpenCL, but only has particle swarm and simulated annealing search. Doesn't talk about sample counts.},
  AUTHOR = {Nugteren, Cedric and Codreanu, Valeriu},
  BOOKTITLE = {MCSoC},
  DOI = {10.1109/MCSoC.2015.10},
  FILE = {:Users/cec/Google Drive/Mendeley/2015 - Nugteren, Codreanu.pdf:pdf},
  ISBN = {978-1-4799-8670-5},
  TITLE = {{ CLTune: A Generic Auto-Tuner for OpenCL Kernels }},
  YEAR = {2015},
}

@ARTICLE{Johnston2015a,
  AUTHOR = {Johnston, Travis and Alsulmi, Mohammad and Cicotti, Pietro and Taufer, Michela},
  FILE = {:Users/cec/Google Drive/Mendeley/2015 - Johnston et al.pdf:pdf},
  JOURNALTITLE = {ICCS},
  PAGES = {49--59},
  TITLE = {{ Performance Tuning of MapReduce Jobs Using Surrogate-Based Modeling }},
  YEAR = {2015},
}

@INPROCEEDINGS{Falch2015,
  ABSTRACT = {Heterogeneous computing, which combines devices with different architectures, is rising in popularity, and promises increased performance combined with reduced energy consumption. OpenCL has been proposed as a standard for programing such systems, and offers func- tional portability. It does, however, suffer from poor per- formance portability, code tuned for one device must be re-tuned to achieve good performance on another device. In this paper, we use machine learning-based auto-tuning to address this problem. Benchmarks are run on a random subset of the entire tuning parameter configuration space, and the results are used to build an artificial neural net- work based model. The model can then be used to find interesting parts of the parameter space for further search. We evaluate our method with different benchmarks, on several devices, including an Intel i7 3770 CPU, an Nvidia K40 GPU and an AMD Radeon HD 7970 GPU. Our model achieves a mean relative error as low as 6.1 { \% } , and is able to find configurations as little as 1.3 { \% } worse than the global minimum.},
  ANNOTATION = {Only three benchmarks, each hand-parameterised. Parameters},
  AUTHOR = {Falch, T. L. and Elster, A. C.},
  BOOKTITLE = {IPDPSW},
  FILE = {:Users/cec/Google Drive/Mendeley/2010 - Adhianto et al.pdf:pdf},
  KEYWORDS = {OpenCL,artificial neural networks,auto-tuning,heterogeneous computing,machine learning},
  PUBLISHER = {IEEE},
  TITLE = {{ Machine Learning Based Auto-tuning for Enhanced OpenCL Performance Portability }},
  YEAR = {2015},
}

@ARTICLE{Kamil2010,
  ABSTRACT = {Although stencil auto-tuning has shown tremendous potential in effectively utilizing architectural resources, it has hitherto been limited to single kernel instantiations; in addition, the large variety of stencil kernels used in practice makes this computation pattern difficult to assemble into a library. This work presents a stencil auto-tuning framework that significantly advances programmer productivity by automatically converting a straightforward sequential Fortran 95 stencil expression into tuned parallel implementations in Fortran, C, or CUDA, thus allowing performance portability across diverse computer architectures, including the AMD Barcelona, Intel Nehalem, Sun Victoria Falls, and the latest NVIDIA GPUs. Results show that our generalized methodology delivers significant performance gains of up to 22Ãƒ speedup over the reference serial implementation. Overall we demonstrate that such domain-specific auto-tuners hold enormous promise for architectural efficiency, programmer productivity, performance portability, and algorithmic adaptability on existing and emerging multicore systems.},
  ANNOTATION = {Kamil presents an auto-tuning framework which accepts as input a Fortran 95 stencil expression, and generates tuned parallel implementations in Fortan, C, or CUDA. The system uses an IR to explore auto-tuning transformations, and has an SMP backend code generator. They demonstrate their system on 4 architectures using 3 benchmarks, with speedups of up to x22 over serial. The CUDA code generator *only uses global memory*. Also, there's no real search engine. They randomly enumerate a subset of the optimisation space, and then record only a *single execution time*, reporting the fastest. Cited by 127.},
  AUTHOR = {Kamil, Shoaib and Chan, Cy and Oliker, Leonid and Shall, John and Williams, Samuel},
  DOI = {10.1109/IPDPS.2010.5470421},
  FILE = {:Users/cec/Google Drive/Mendeley/2010 - Kamil et al.pdf:pdf},
  ISBN = {9781424464432},
  ISSN = {15302075},
  JOURNALTITLE = {IPDPS},
  TITLE = {{ An auto-tuning framework for parallel multicore stencil computations }},
  YEAR = {2010},
}

@INPROCEEDINGS{Chiu2015,
  ABSTRACT = {We describe Genesis, a language for the generation of syn- thetic programs for use in machine learning-based perfor- mance auto-tuning. The language allows users to annotate a template program to customize its code using statistical distributions and to generate program instances based on those distributions. This effectively allows users to generate training programs whose characteristics or features vary in a statistically controlled fashion. We describe the language constructs, a prototype preprocessor for the language, and three case studies that show the ability of Genesis to ex- press a range of training programs in different domains. We evaluate the preprocessor's performance and the statistical quality of the samples it generates. We believe that Gen- esis is a useful tool for generating large and diverse sets of programs, a necessary component when training machine learning models for auto-tuning.},
  ANNOTATION = {From Duplicate 2 (Genesis: A Language for Generating Synthetic Training Programs for Machine Learning - Chiu, Alton; Garvey, Joseph; Abdelrahman, Tarek S) 0 cites.},
  AUTHOR = {Chiu, A. and Garvey, J. and Abdelrahman, T. S.},
  BOOKTITLE = {CF},
  DOI = {10.1145/2742854.2742883},
  FILE = {:Users/cec/Google Drive/Mendeley/2015 - Chiu, Garvey, Abdelrahman.pdf:pdf},
  ISBN = {9781450333580},
  KEYWORDS = {macro languages,synthetic program generation},
  PAGES = {8},
  PUBLISHER = {ACM},
  TITLE = {{ Genesis: A Language for Generating Synthetic Training Programs for Machine Learning }},
  YEAR = {2015},
}

@INPROCEEDINGS{Leather2009,
  ABSTRACT = {Many problems in embedded compilation require one set of op- timizations to be selected over another based on run time per- formance. Self-tuned libraries, iterative compilation and machine learning techniques all compare multiple compiled program ver- sions. In each, program versions are timed to determine which has the best performance. The program needs to be run multiple times for each version because there is noise inherent in most performance measurements. The number of runs must be enough to compare different versions, despite the noise, but executing more than this will waste time and energy. The compiler writer must either risk taking too few runs, potentially getting incorrect results, or taking too many runs increasing the time for their experiments or reducing the number of program versions evaluated. Prior works choose constant size sampling plans where each compiled version is executed a fixed number of times without regard to the level of noise. In this paper we develop a sequential sampling plan which can automatically adapt to the experiment so that the compiler writer can have both confidence in the results and also be sure that no more runs were taken than were needed.We show that our system is able to correctly determine the best optimization settings with between 76 { \% } and 87 { \% } fewer runs than needed by a brute force, constant sampling size approach.We also compare our approach to JavaSTATS(10); we needed 77 { \% } to 89 { \% } fewer runs than it needed.},
  AUTHOR = {Leather, Hugh and O'Boyle, Michael and Worton, Bruce},
  BOOKTITLE = {LCTES},
  FILE = {:Users/cec/Google Drive/Mendeley/2009 - Leather, O'Boyle, Worton.pdf:pdf},
  PUBLISHER = {ACM},
  TITLE = {{ Raced Profiles: Efficient Selection of Competing Compiler Optimizations }},
  YEAR = {2009},
}

